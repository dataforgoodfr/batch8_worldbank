{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with NOAA dataset\n",
    "\n",
    "The journal article describing GHCN-Daily is:\n",
    "\n",
    "Menne, M.J., I. Durre, R.S. Vose, B.E. Gleason, and T.G. Houston, 2012: An overview of the Global Historical Climatology Network-Daily Database. Journal of Atmospheric and Oceanic Technology, 29, 897-910, doi:10.1175/JTECH-D-11-00103.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple output per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = '/media/data-nvme/dev/datasets/WorldBank/'\n",
    "noaa_csv_path = DATASET_FOLDER + 'noaa/ASN*.csv'\n",
    "SPARK_MASTER = 'spark://192.168.0.9:7077'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624444"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#noaa_csv_path = '/media/data-nvme/dev/datasets/WorldBank//noaa/ASN00060066.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/ASN*.csv'\n",
    "noaa_csv_path = DATASET_FOLDER + '/france/*.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/small_dataset/*.csv'\n",
    "def count_lines():\n",
    "    # configuration\n",
    "    APP_NAME = 'count NOAA all lines'\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    conf = conf.setMaster(SPARK_MASTER)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # core part of the script\n",
    "    files = sc.textFile(noaa_csv_path)\n",
    "    total = files.count()\n",
    "#     lineLength = lines.map(lambda s: len(s))\n",
    "#     lineLength.persist()\n",
    "#     totalLength = lineLength.reduce(lambda a,b:a+b)\n",
    "#     # output results\n",
    "#     print(totalLength)\n",
    "    sc.stop()\n",
    "    return total\n",
    "\n",
    "total = count_lines()\n",
    "#print(f'{totalLength:%.2f}')\n",
    "total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624,444\n"
     ]
    }
   ],
   "source": [
    "print(f'{total:,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of lines in the full dataset : 1 076 166 433\n",
    "\n",
    "# Filters\n",
    "\n",
    "https://docs.opendata.aws/noaa-ghcn-pds/readme.html\n",
    "\n",
    "https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt\n",
    "\n",
    "Keep only \n",
    "\n",
    "ELEMENT Summary\n",
    "\n",
    "The five core elements are:\n",
    "\n",
    "    PRCP = Precipitation (tenths of mm)\n",
    "    SNOW = Snowfall (mm)\n",
    "    SNWD = Snow depth (mm)\n",
    "    TMAX = Maximum temperature (tenths of degrees C)\n",
    "    TMIN = Minimum temperature (tenths of degrees C)\n",
    "\n",
    "    Variable   Columns   Type\n",
    "    ------------------------------\n",
    "    ID            1-11   Character\n",
    "    YEAR         12-15   Integer\n",
    "    MONTH        16-17   Integer\n",
    "    ELEMENT      18-21   Character\n",
    "    VALUE1       22-26   Integer\n",
    "    MFLAG1       27-27   Character\n",
    "    QFLAG1       28-28   Character\n",
    "\t\t   \n",
    "VALUE1     is the value on the first day of the month (missing = -9999).\n",
    "\n",
    "      \"PRCP_ATTRIBUTES\" = a,M,Q,S where:\n",
    "           a = DaysMissing (Numeric value): The number of days (from 1 to 5) missing or flagged is provided   \n",
    "           M = GHCN-Daily Dataset Measurement Flag (see Section 1.3.a.ii for more details) \n",
    "           Q = GHCN-Daily Dataset Quality Flag (see Section 1.3.a.iii for more details)\n",
    "           S = GHCN-Daily Dataset Source Code (see Section 1.3.a.iv for more details)  \n",
    "\n",
    "MFLAG1     is the measurement flag for the first day of the month.  There are\n",
    "           ten possible values:\n",
    "\n",
    "           Blank = no measurement information applicable\n",
    "           B     = precipitation total formed from two 12-hour totals\n",
    "           D     = precipitation total formed from four six-hour totals\n",
    "\t   H     = represents highest or lowest hourly temperature (TMAX or TMIN) \n",
    "\t           or the average of hourly values (TAVG)\n",
    "\t   K     = converted from knots \n",
    "\t   L     = temperature appears to be lagged with respect to reported\n",
    "\t           hour of observation \n",
    "           O     = converted from oktas \n",
    "\t   P     = identified as \"missing presumed zero\" in DSI 3200 and 3206\n",
    "           T     = trace of precipitation, snowfall, or snow depth\n",
    "\t   W     = converted from 16-point WBAN code (for wind direction)\n",
    "\n",
    "QFLAG1     is the quality flag for the first day of the month.  There are \n",
    "           fourteen possible values:\n",
    "\n",
    "           Blank = did not fail any quality assurance check\n",
    "           D     = failed duplicate check\n",
    "           G     = failed gap check\n",
    "           I     = failed internal consistency check\n",
    "           K     = failed streak/frequent-value check\n",
    "\t   L     = failed check on length of multiday period \n",
    "           M     = failed megaconsistency check\n",
    "           N     = failed naught check\n",
    "           O     = failed climatological outlier check\n",
    "           R     = failed lagged range check\n",
    "           S     = failed spatial consistency check\n",
    "           T     = failed temporal consistency check\n",
    "           W     = temperature too warm for snow\n",
    "           X     = failed bounds check\n",
    "\t   Z     = flagged as a result of an official Datzilla \n",
    "\t           investigation\n",
    "    \n",
    "- WESF = Water equivalent of snowfall (tenths of mm)\n",
    "\n",
    "WV** = Weather in the Vicinity where ** has one of the following values:\n",
    "\n",
    "    01 = Fog, ice fog, or freezing fog (may include heavy fog)\n",
    "    03 = Thunder\n",
    "    07 = Ash, dust, sand, or other blowing obstruction\n",
    "    18 = Snow or ice crystals\n",
    "    20 = Rain or snow shower\n",
    "\n",
    "WMO ID is the World Meteorological Organization (WMO) number for the station. If the station has no WMO number (or one has not yet been matched to this station), then the field is blank.\n",
    "\n",
    "     \n",
    "\n",
    "HCN/CRN FLAG = flag that indicates whether the station is part of the U.S. Historical Climatology Network (HCN). There are three possible values:\n",
    "\n",
    "    Blank = Not a member of the U.S. Historical Climatology or U.S. Climate Reference Networks\n",
    "    HCN = U.S. Historical Climatology Network station\n",
    "    CRN = U.S. Climate Reference Network or U.S. Regional Climate Network Station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: ligne 0: fin de fichier (EOF) prématurée lors de la recherche du « ' » correspondant\n",
      "/bin/bash: -c: ligne 1: erreur de syntaxe : fin de fichier prématurée\n"
     ]
    }
   ],
   "source": [
    "!head /media/data-nvme/dev/datasets/WorldBank//noaa/ASN00060066.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $DATASET_FOLDER/noaa/ASN00060066.csv\n",
    "!wc -l $DATASET_FOLDER/noaa/ASN00060066.csv\n",
    "!grep 2016 $DATASET_FOLDER/noaa/AE000041196.csv | head -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"TAVG\",\"TAVG_ATTRIBUTES\"\n",
      "\"FRM00007180\",\"1944-11-03\",\"48.692\",\"6.23\",\"228.9\",\"ESSEY, FR\",,,,,,,,,\"   49\",\"H,,S\"\n",
      "\"FRM00007180\",\"1944-11-04\",\"48.692\",\"6.23\",\"228.9\",\"ESSEY, FR\",,,,,,,,,\"   58\",\"H,,S\"\n"
     ]
    }
   ],
   "source": [
    "!head -3 /media/data-nvme/dev/datasets/WorldBank/france/FRM00007180.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /media/data-nvme/dev/datasets/WorldBank/year_2016-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1944-'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s ='\"FRM00007180\",\"1944-11-03\",\"48.692\",\"6.23\",\"228.9\",\"ESSEY, FR\",,,,,,,,,\"   49\",\"H,,S\"'\n",
    "s[15:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter on Rain - Join version\n",
    "\n",
    "113 074 files with rain data, no need to filter before other operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o458.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:953)\n\tat jdk.internal.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 61.0 failed 4 times, most recent failure: Lost task 11.3 in stage 61.0 (TID 42443, 192.168.0.9, executor 7): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 32 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/spark/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1025\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/spark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/spark/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/spark/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o458.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:953)\n\tat jdk.internal.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 61.0 failed 4 times, most recent failure: Lost task 11.3 in stage 61.0 (TID 42443, 192.168.0.9, executor 7): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 32 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import shutil\n",
    "\n",
    "DATASET_FOLDER = '/media/data-nvme/dev/datasets/WorldBank/'\n",
    "SPARK_MASTER = 'spark://192.168.0.9:7077'\n",
    "APP_NAME = 'Group daily rain by Country'\n",
    "\n",
    "noaa_csv_path = DATASET_FOLDER + '/noaa/*.csv'\n",
    "noaa_csv_path = DATASET_FOLDER + 'noaa/ASN*.csv'\n",
    "output = DATASET_FOLDER + 'daily_rain_by_country'\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.master(SPARK_MASTER).appName(APP_NAME).getOrCreate()\n",
    "# Convert list to data frame\n",
    "df = spark.read.format('csv').option('header',True).option('multiLine', True).load(noaa_csv_path)\n",
    "df_rain = df[[\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\"]]\n",
    "\n",
    "# Extract country code\n",
    "df_rain = df_rain.withColumn('COUNTRY',  F.col('NAME').substr(F.length('NAME')-1, F.length('NAME')))\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df_rain.createOrReplaceTempView(\"noaa\")\n",
    "sqlDF = spark.sql(\"SELECT DATE, COUNTRY, ceil(100 * avg(PRCP))/100 as avg_PRCP, ceil(100 * sum(PRCP))/100 as sum_PRCP, max(PRCP) as max_PRCP, ceil(100 * stddev(PRCP))/100 as stddev_PRCP, count(PRCP) as count_PRCP FROM noaa GROUP BY DATE, COUNTRY;\")\n",
    "\n",
    "shutil.rmtree(output, ignore_errors=True)\n",
    "#sqlDF.repartition(1).write.csv(output)\n",
    "sqlDF.write.csv(output)\n",
    "print(sqlDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /media/data-nvme/dev/datasets/WorldBank/year_2016-01_rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count is: 624408\n",
      "CPU times: user 2.53 ms, sys: 3.69 ms, total: 6.21 ms\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/*.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + 'year_2016-fra'\n",
    "noaa_csv_path = DATASET_FOLDER + '/france/*.csv'\n",
    "inventory_path = DATASET_FOLDER + 'ghcnd-inventory.txt'\n",
    "output = DATASET_FOLDER + 'france_rain'\n",
    "#sc.stop()\n",
    "total = 0\n",
    "# configuration\n",
    "APP_NAME = 'Filter on Rain'\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.master(SPARK_MASTER).appName(APP_NAME).getOrCreate()\n",
    "# Convert list to data frame\n",
    "df = spark.read.format('csv').option('header',True).option('multiLine', True).load(noaa_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"TAVG\",\"TAVG_ATTRIBUTES'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\",\"'.join(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STATION',\n",
       " 'DATE',\n",
       " 'LATITUDE',\n",
       " 'LONGITUDE',\n",
       " 'ELEVATION',\n",
       " 'NAME',\n",
       " 'PRCP',\n",
       " 'PRCP_ATTRIBUTES']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rain = df[[\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(STATION='FRM00007207', DATE='1930-04-01', LATITUDE='47.2942', LONGITUDE='-3.2181', ELEVATION='34.0', NAME='BELLE ILE LE TALUT, FR', PRCP='   20', PRCP_ATTRIBUTES=',,E', COUNTRY='FR'),\n",
       " Row(STATION='FRM00007207', DATE='1930-04-02', LATITUDE='47.2942', LONGITUDE='-3.2181', ELEVATION='34.0', NAME='BELLE ILE LE TALUT, FR', PRCP='  122', PRCP_ATTRIBUTES=',,E', COUNTRY='FR'),\n",
       " Row(STATION='FRM00007207', DATE='1930-04-03', LATITUDE='47.2942', LONGITUDE='-3.2181', ELEVATION='34.0', NAME='BELLE ILE LE TALUT, FR', PRCP='   93', PRCP_ATTRIBUTES=',,E', COUNTRY='FR')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_rain[\"COUNTRY\"] = df_rain[\"NAME\"][:-2]\n",
    "#from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Extract country code\n",
    "df_rain = df_rain.withColumn('COUNTRY',  F.col('NAME').substr(F.length('NAME')-1, F.length('NAME')))\n",
    "df_rain.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STATION',\n",
       " 'DATE',\n",
       " 'LATITUDE',\n",
       " 'LONGITUDE',\n",
       " 'ELEVATION',\n",
       " 'NAME',\n",
       " 'PRCP',\n",
       " 'PRCP_ATTRIBUTES',\n",
       " 'COUNTRY']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = DATASET_FOLDER + 'france_rain'\n",
    "df_rain.repartition(1).write.csv(output)\n",
    "#df_rain.write.csv(output)\n",
    "df_rain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>PRCP_ATTRIBUTES</th>\n",
       "      <th>COUNTRY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRM00007207</td>\n",
       "      <td>1930-04-01</td>\n",
       "      <td>47.2942</td>\n",
       "      <td>-3.2181</td>\n",
       "      <td>34.0</td>\n",
       "      <td>BELLE ILE LE TALUT, FR</td>\n",
       "      <td>20.0</td>\n",
       "      <td>,,E</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FRM00007207</td>\n",
       "      <td>1930-04-02</td>\n",
       "      <td>47.2942</td>\n",
       "      <td>-3.2181</td>\n",
       "      <td>34.0</td>\n",
       "      <td>BELLE ILE LE TALUT, FR</td>\n",
       "      <td>122.0</td>\n",
       "      <td>,,E</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FRM00007207</td>\n",
       "      <td>1930-04-03</td>\n",
       "      <td>47.2942</td>\n",
       "      <td>-3.2181</td>\n",
       "      <td>34.0</td>\n",
       "      <td>BELLE ILE LE TALUT, FR</td>\n",
       "      <td>93.0</td>\n",
       "      <td>,,E</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATION        DATE  LATITUDE  LONGITUDE  ELEVATION  \\\n",
       "0  FRM00007207  1930-04-01   47.2942    -3.2181       34.0   \n",
       "1  FRM00007207  1930-04-02   47.2942    -3.2181       34.0   \n",
       "2  FRM00007207  1930-04-03   47.2942    -3.2181       34.0   \n",
       "\n",
       "                     NAME   PRCP PRCP_ATTRIBUTES COUNTRY  \n",
       "0  BELLE ILE LE TALUT, FR   20.0             ,,E      FR  \n",
       "1  BELLE ILE LE TALUT, FR  122.0             ,,E      FR  \n",
       "2  BELLE ILE LE TALUT, FR   93.0             ,,E      FR  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_out = pd.read_csv('/media/data-nvme/dev/datasets/WorldBank/france-rain.csv')\n",
    "df_out.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>PRCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43.828025</td>\n",
       "      <td>4.259108</td>\n",
       "      <td>197.538889</td>\n",
       "      <td>43.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.152703</td>\n",
       "      <td>11.882491</td>\n",
       "      <td>222.977693</td>\n",
       "      <td>59.728666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-49.350000</td>\n",
       "      <td>-5.050000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>44.065750</td>\n",
       "      <td>0.086250</td>\n",
       "      <td>55.475000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46.869000</td>\n",
       "      <td>2.700600</td>\n",
       "      <td>113.550000</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.508000</td>\n",
       "      <td>5.301500</td>\n",
       "      <td>262.425000</td>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.562000</td>\n",
       "      <td>70.250000</td>\n",
       "      <td>876.000000</td>\n",
       "      <td>201.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        LATITUDE  LONGITUDE   ELEVATION        PRCP\n",
       "count  36.000000  36.000000   36.000000   36.000000\n",
       "mean   43.828025   4.259108  197.538889   43.972222\n",
       "std    16.152703  11.882491  222.977693   59.728666\n",
       "min   -49.350000  -5.050000    3.700000    0.000000\n",
       "25%    44.065750   0.086250   55.475000    0.000000\n",
       "50%    46.869000   2.700600  113.550000   12.500000\n",
       "75%    48.508000   5.301500  262.425000   78.000000\n",
       "max    50.562000  70.250000  876.000000  201.000000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_out.query('DATE == \"2016-04-01\"')\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df_rain.createOrReplaceTempView(\"noaa\")\n",
    "sqlDF = spark.sql(\"SELECT DATE, COUNTRY, ceil(100 * avg(PRCP))/100 as avg_PRCP, ceil(100 * sum(PRCP))/100 as sum_PRCP, max(PRCP) as max_PRCP, ceil(100 * stddev(PRCP))/100 as stddev_PRCP, count(PRCP) as count_PRCP FROM noaa GROUP BY DATE, COUNTRY;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DATE='1922-04-17', COUNTRY='FR', avg_PRCP=24.0, sum_PRCP=24.0, max_PRCP='   24', stddev_PRCP=0.0, count_PRCP=1),\n",
       " Row(DATE='1923-01-13', COUNTRY='FR', avg_PRCP=0.0, sum_PRCP=0.0, max_PRCP='    0', stddev_PRCP=0.0, count_PRCP=1),\n",
       " Row(DATE='1923-03-20', COUNTRY='FR', avg_PRCP=0.0, sum_PRCP=0.0, max_PRCP='    0', stddev_PRCP=0.0, count_PRCP=1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlDF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATE',\n",
       " 'COUNTRY',\n",
       " 'avg_PRCP',\n",
       " 'sum_PRCP',\n",
       " 'max_PRCP',\n",
       " 'stddev_PRCP',\n",
       " 'count_PRCP']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = DATASET_FOLDER + 'france_rain_avg_by_day'\n",
    "shutil.rmtree(output)\n",
    "sqlDF.repartition(1).write.csv(output)\n",
    "#df_rain.write.csv(output)\n",
    "sqlDF.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DATE,COUNTRY,avg_PRCP,sum_PRCP,max_PRCP,stddev_PRCP,count_PRCP'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(sqlDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "full_df = sc.read.option(\"header\", True).option(\"inferSchema\", True).csv(noaa_csv_path)\n",
    "\n",
    "# Load inventory\n",
    "inventory_rdd = sc.textFile(inventory_path)\n",
    "# Filter on Rain\n",
    "rain_inventory_rdd = inventory_rdd.filter(lambda s : \"PRCP\" in s)\n",
    "# Now we have a list of all files containings precipitation data\n",
    "# Keep only the first column\n",
    "# Format each RDD as (K, V) to prepare for the join operation\n",
    "rain_inventory_rdd = rain_inventory_rdd.map(lambda line : (line[0:11], line[36:])) # Keep code and years\n",
    "#rain_inventory_rdd = rain_inventory_rdd.filter(lambda s: s[0:11])\n",
    "#rain_inventory = rain_inventory_rdd.collect()\n",
    "# Load the stations data points\n",
    "print(f'Processing {noaa_csv_path}...')\n",
    "all_data_rdd = sc.textFile(noaa_csv_path+'/*')\n",
    "all_data_rdd = all_data_rdd.map(lambda line: (line[1:12], line[14:]))\n",
    "# Keep only precipitation data\n",
    "join = rain_inventory_rdd.join(all_data_rdd)\n",
    "#rain_rdd  = files_rdd.filter(lambda s : s[1:12] in rain_inventory)\n",
    "# Save precipitation data\n",
    "\n",
    "print(f'Saving to {output}')\n",
    "join_output = join.map(lambda x: ','.join([x[0],x[1][1]]))\n",
    "#sc.parallelize(join_output.take(2)).collect()\n",
    "# Flatten the result\n",
    "#join = rdd.map(lambda x: ','.join([x[0],x[1][0],x[1][1]]))\n",
    "# Get all partition on one node, to have one file (don't do it for huge dataset)\n",
    "join_output = join_output.repartition(1)\n",
    "\n",
    "shutil.rmtree(output, ignore_errors=True)\n",
    "join_output.saveAsTextFile(output)\n",
    "total = join.count()\n",
    "sc.stop()\n",
    "\n",
    "total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter on Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /media/data-nvme/dev/datasets/WorldBank//france/*.csv...\n",
      "Saving to /media/data-nvme/dev/datasets/WorldBank/year_2016-fra\n",
      "CPU times: user 28.6 ms, sys: 5.06 ms, total: 33.7 ms\n",
      "Wall time: 5.43 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13105"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#noaa_csv_path = '/media/data-nvme/dev/datasets/WorldBank//noaa/ASN00060066.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/AE*.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/*.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/small_dataset/*.csv'\n",
    "noaa_csv_path = DATASET_FOLDER + '/france/*.csv'\n",
    "def filter_year():\n",
    "    total = 0\n",
    "    # configuration\n",
    "    APP_NAME = 'Filter on Year'\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    conf = conf.setMaster(SPARK_MASTER)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    try:\n",
    "        # core part of the script\n",
    "        print(f'Processing {noaa_csv_path}...')\n",
    "        files_rdd = sc.textFile(noaa_csv_path)\n",
    "        year_2016  = files_rdd.filter(lambda s : s[15:20] == \"2016-\" in s)\n",
    "        output = DATASET_FOLDER + 'year_2016-fra'\n",
    "        print(f'Saving to {output}')\n",
    "        year_2016.saveAsTextFile(output)\n",
    "        total = year_2016.count()\n",
    "    except Exception as inst:\n",
    "        print('ERROR')\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)     # arguments stored in .args\n",
    "        print(inst)\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()\n",
    "        return total\n",
    "\n",
    "total = filter_year()\n",
    "#print(f'{totalLength:%.2f}')\n",
    "total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '\"USR0000AFRA\",\"2016-01-25\",\"35.8456\",\"-113.055\",\"2063.5\",\"FRAZIER WELLS ARIZONA, AZ US\",\"   39\",\"H,,U\",\"  -39\",\"H,,U\",\"  -10\",\",,U\"'\n",
    "s[1:12]\n",
    "s[14:]\n",
    "s = 'ACW00011604  17.1167  -61.7833 TMAX 1949 1949'\n",
    "s[0:11]\n",
    "s[36:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/56957589/how-to-read-multiple-csv-files-with-different-schema-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"TAVG\",\"TAVG_ATTRIBUTES\"\n",
      "FRM00007005,\"2016-01-01\",\"50.143\",\"1.832\",\"67.1\",\"ABBEVILLE, FR\",\"    5\",\",,S\",,,\"   88\",\",,S\",\"   21\",\",,S\",\"   56\",\"H,,S\"\n",
      "FRM00007180,\"2016-01-01\",\"48.692\",\"6.23\",\"228.9\",\"ESSEY, FR\",\"   15\",\",,S\",,,\"   60\",\",,S\",\"    9\",\",,S\",\"   44\",\"H,,S\"\n",
      "FRM00007535,\"2016-01-01\",\"44.745\",\"1.3967\",\"260.0\",\"GOURDON, FR\",\"   12\",\",,E\",,,\"  137\",\",,E\",\"    9\",\",,E\",\"   84\",\"H,,S\"\n",
      "FRM00007607,\"2016-01-01\",\"43.912\",\"-0.508\",\"61.9\",\"MONT DE MARSAN, FR\",\"    5\",\",,S\",,,,,\"    9\",\",,S\",\"   77\",\"H,,S\"\n",
      "FRM00007240,\"2016-01-01\",\"47.432\",\"0.728\",\"108.8\",\"VAL DE LOIRE, FR\",\"    0\",\",,S\",,,\"   96\",\",,S\",,,\"   68\",\"H,,S\"\n",
      "FRM00007790,\"2016-01-01\",\"42.553\",\"9.484\",\"7.9\",\"PORETTA, FR\",\"    0\",\",,S\",,,\"  133\",\",,S\",,,\"  107\",\"H,,S\"\n",
      "FRM00007558,\"2016-01-01\",\"44.117\",\"3.017\",\"720.0\",\"MILLAU, FR\",\"   48\",\",,S\",,,,,\"   37\",\",,S\",\"   66\",\"H,,S\"\n",
      "FRM00007621,\"2016-01-01\",\"43.179\",\"-0.006\",\"384.0\",\"LOURDES, FR\",\"    5\",\",,S\",,,\"  141\",\",,S\",,,\"   76\",\"H,,S\"\n",
      "FRM00007335,\"2016-01-01\",\"46.588\",\"0.307\",\"128.9\",\"BIARD, FR\",\"    3\",\",,S\",,,,,\"   45\",\",,S\",\"   77\",\"H,,S\"\n",
      "FRM00061998,\"2016-01-01\",\"-49.35\",\"70.25\",\"30.0\",\"PORT AUX FRANCAIS ILES KERGU, FR\",\"    3\",\",,S\",,,\"  126\",\",,S\",\"   38\",\",,S\",\"   76\",\"H,,S\"\n"
     ]
    }
   ],
   "source": [
    "!echo '\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"TAVG\",\"TAVG_ATTRIBUTES\"'\n",
    "!head $DATASET_FOLDER/year_2016-01_rain/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FRM00007005',\n",
       "  ('1973 2020',\n",
       "   '\"2016-01-01\",\"50.143\",\"1.832\",\"67.1\",\"ABBEVILLE, FR\",\"    5\",\",,S\",,,\"   88\",\",,S\",\"   21\",\",,S\",\"   56\",\"H,,S\"')),\n",
       " ('FRM00007180',\n",
       "  ('1973 2020',\n",
       "   '\"2016-01-01\",\"48.692\",\"6.23\",\"228.9\",\"ESSEY, FR\",\"   15\",\",,S\",,,\"   60\",\",,S\",\"    9\",\",,S\",\"   44\",\"H,,S\"'))]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(join.take(2)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRM00007005,\"2016-01-01\",\"50.143\",\"1.832\",\"67.1\",\"ABBEVILLE, FR\",\"    5\",\",,S\",,,\"   88\",\",,S\",\"   21\",\",,S\",\"   56\",\"H,,S\"',\n",
       " 'FRM00007180,\"2016-01-01\",\"48.692\",\"6.23\",\"228.9\",\"ESSEY, FR\",\"   15\",\",,S\",,,\"   60\",\",,S\",\"    9\",\",,S\",\"   44\",\"H,,S\"']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"ORLY\" ID : 0-20000-0-07149\n",
    "\n",
    "Coordinates: 48.7166666667°N, 2.3844444444°E, 89m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd $DATASET_FOLDER/year_2016-01_rain && (ls | xargs cat)  > ../year_2016-01_rain.csv\n",
    "# ! head $DATASET_FOLDER/year_2016-01_rain.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noaa_csv_path = '/media/data-nvme/dev/datasets/WorldBank//noaa/ASN00060066.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/AE*.csv'\n",
    "#noaa_csv_path = DATASET_FOLDER + '/noaa/*.csv'\n",
    "noaa_csv_path = DATASET_FOLDER + 'year_2016-01.csv'\n",
    "def extract_rain():\n",
    "    total = 0\n",
    "    # configuration\n",
    "    APP_NAME = 'Count Rain'\n",
    "    conf = SparkConf().setAppName(APP_NAME)\n",
    "    conf = conf.setMaster(SPARK_MASTER)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    try:\n",
    "        # core part of the script\n",
    "        print(f'Processing {noaa_csv_path}...')\n",
    "        files_rdd = sc.textFile(noaa_csv_path+'/*')\n",
    "        total = files_rdd.count()\n",
    "    except Exception as inst:\n",
    "        print('ERROR')\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)     # arguments stored in .args\n",
    "        print(inst)\n",
    "        raise\n",
    "    finally:\n",
    "        sc.stop()\n",
    "        return total\n",
    "\n",
    "total = extract_rain()\n",
    "#print(f'{totalLength:%.2f}')\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head $DATASET_FOLDER/year_2016.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "customSchema = StructType([ \\\n",
    "StructField(\"asset_id\", StringType(), True), \\\n",
    "StructField(\"price_date\", StringType(), True), \\\n",
    "etc., \n",
    "StructField(\"close_price\", StringType(), True), \\\n",
    "StructField(\"filename\", StringType(), True)])\n",
    "\n",
    "\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "   .option(\"header\", \"false\") \\\n",
    "   .option(\"sep\",\"|\") \\\n",
    "   .schema(customSchema) \\\n",
    "   .load(fullPath) \\\n",
    "   .withColumn(\"filename\", input_file_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"USR0000AFRA\",\"2016-01-25\",\"35.8456\",\"-113.055\",\"2063.5\",\"FRAZIER WELLS ARIZONA, AZ US\",\"   39\",\"H,,U\",\"  -39\",\"H,,U\",\"  -10\",\",,U\"',\n",
       " '\"FRM00007149\",\"2016-01-01\",\"48.7167\",\"2.3842\",\"89.0\",\"ORLY, FR\",\"   10\",\",,E\",,,\"   85\",\",,E\",\"   38\",\",,E\",\"   62\",\"H,,S\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('test')\n",
    "conf = conf.setMaster(SPARK_MASTER)\n",
    "sc = SparkContext(conf=conf)\n",
    "s = ['\"USR0000AFRA\",\"2016-01-25\",\"35.8456\",\"-113.055\",\"2063.5\",\"FRAZIER WELLS ARIZONA, AZ US\",\"   39\",\"H,,U\",\"  -39\",\"H,,U\",\"  -10\",\",,U\"']\n",
    "s += ['\"FRM00007149\",\"2016-01-01\",\"48.7167\",\"2.3842\",\"89.0\",\"ORLY, FR\",\"   10\",\",,E\",,,\"   85\",\",,E\",\"   38\",\",,E\",\"   62\",\"H,,S\"']\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('USR0000AFR', ('2016-01-25', '35.8456')),\n",
       " ('FRM0000714', ('2016-01-01', '48.7167'))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(s)\n",
    "#test = rain_inventor_rdd.map(lambda line : (line[0:11], line[36:])) # Keep code and years\n",
    "rdd = rdd.map(lambda line : (line[1:11], (line[15:25], line[28:35])))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('USR0000AFR', ('2016-01-25', '35.8456')),\n",
       " ('FRM0000714', ('2016-01-01', '48.7167'))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USR0000AFR,2016-01-25,35.8456', 'FRM0000714,2016-01-01,48.7167']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: ','.join([x[0],x[1][0],x[1][1]])).repartition(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
